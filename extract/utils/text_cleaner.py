import re
from typing import List
from utils.korean_rules import JOSA, EOMI


def clean_text(text: str) -> str:
    # ì´ëª¨ì§€, íŠ¹ìˆ˜ë¬¸ì ì œê±°
    return re.sub(r"[^\w\sã„±-í£]", "", text).strip()


def remove_josa_and_eomi(token: str) -> str:
    """
    ì¡°ì‚¬, ì–´ë¯¸ ë“±ì„ ë°˜ë³µì ìœ¼ë¡œ ì œê±°í•´ì„œ í•µì‹¬ ë‹¨ì–´ë¥¼ ë°˜í™˜
    ì˜ˆ: 'ë°°ê°€ìš”' â†’ 'ë°°', 'ì•„í”„ê³ ' â†’ 'ì•„í”„'
    """

    while True:
        original = token
        print(f"ğŸ§ª [ì •ì œ ì „] {token}", end=" â†’ ")
        for j in sorted(JOSA, key=len, reverse=True):
            if token.endswith(j):
                token = token[: -len(j)]
                break
        for e in sorted(EOMI, key=len, reverse=True):
            if token.endswith(e):
                token = token[: -len(e)]
                break
        if token == original:
            break
        print(f"[ì •ì œ í›„] {token}")
    return token


def clean_and_tokenize(text: str) -> List[str]:
    """
    ë¬¸ì¥ì„ ì •ì œí•˜ê³ , ì¡°ì‚¬/ì–´ë¯¸ ì œê±° í›„ í•µì‹¬ ë‹¨ì–´ë¡œ êµ¬ì„±ëœ í† í° ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    """
    cleaned = clean_text(text)
    raw_tokens = cleaned.split()

    whitelist = {"ë°°", "í”¼", "ì—´", "ëª©", "ì†", "ëˆˆ", "ê·€", "ì½”", "íŒ”", "ìœ„", "ëª¸"}
    tokens: List[str] = []

    for token in raw_tokens:
        stripped = remove_josa_and_eomi(token)
        if len(stripped) > 1 or stripped in whitelist:
            tokens.append(stripped)

    return tokens
