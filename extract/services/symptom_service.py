from typing import List, Dict, Optional
from utils.symptom_mapping import SYMPTOM_MAPPING
from utils.text_cleaner import clean_and_tokenize

TIME_KEYWORDS = {
    "ko": {
        "ì•„ì¹¨": "morning",
        "ì˜¤ì „": "morning",
        "ì ì‹¬": "afternoon",
        "ì˜¤í›„": "afternoon",
        "ì €ë…": "evening",
        "ë°¤": "night",
    },
    "en": {
        "morning": "morning",
        "afternoon": "afternoon",
        "evening": "evening",
        "night": "night",
    },
}


def extract_combined_symptoms(text_ko: str, text_en: str) -> List[Dict[str, str]]:
    print(f"\nğŸ”µ [Step 1] ì›ë¬¸(ko): {text_ko}")
    print(f"ğŸ”µ [Step 2] ë²ˆì—­(en): {text_en}")

    results = []
    tokens_ko = clean_and_tokenize(text_ko)
    tokens_en = clean_and_tokenize(text_en.lower())

    print(f"\nğŸŸ¡ [Step 3] Tokenized (ko): {tokens_ko}")
    print(f"ğŸŸ¡ [Step 4] Tokenized (en): {tokens_en}")

    # âœ… ì •ì œ í›„ í† í° ì¶œë ¥ ì¶”ê°€
    print(f"\nğŸŸ¡ [Step 4-1] ì •ì œ í›„ í† í° (ko): {[t for t in tokens_ko]}")
    print(f"ğŸŸ¡ [Step 4-2] ì •ì œ í›„ í† í° (en): {[t for t in tokens_en]}")

    for symptom, mapping in SYMPTOM_MAPPING.items():
        # 1ï¸âƒ£ í•œê¸€ í‚¤ì›Œë“œ ì§ì ‘ ë§¤ì¹­
        if any(keyword in text_ko for keyword in mapping["ko"]):
            print(f"âœ… [KO match] '{symptom}' matched by keyword in: {mapping['ko']}")
            results.append({"symptom": symptom, "time": detect_time(text_ko, "ko")})
            continue

        # 2ï¸âƒ£ ì˜ì–´ í‚¤ì›Œë“œ ì§ì ‘ ë§¤ì¹­
        if any(keyword in text_en for keyword in mapping["en"]):
            print(f"âœ… [EN match] '{symptom}' matched by keyword in: {mapping['en']}")
            results.append({"symptom": symptom, "time": detect_time(text_en, "en")})
            continue

        # 3ï¸âƒ£ í† í° ê¸°ë°˜ ì¡°í•©ì‹ ë§¤ì¹­
        for token_set in mapping.get("token_sets", []):
            part1_match = [tok for tok in tokens_ko if tok in token_set["part1"]]
            part2_match = [tok for tok in tokens_ko if tok in token_set["part2"]]
            print(
                f"ğŸ” [DEBUG] {symptom} token_sets ê²€ì‚¬ - part1: {part1_match}, part2: {part2_match}"
            )
            if part1_match and part2_match:
                print(
                    f"âœ… [TokenSet match] '{symptom}' matched by part1: {part1_match}, part2: {part2_match}"
                )
                results.append({"symptom": symptom, "time": detect_time(text_ko, "ko")})
                break

    # ğŸ”¹ ë³µí•© ì¦ìƒ (e.g., ëª¸ì‚´) ë¶„í•´ ì²˜ë¦¬
    composite = handle_composite_symptoms(text_ko, results)
    if composite:
        print(
            f"âœ… [Composite] Added symptoms from 'ëª¸ì‚´': {[s['symptom'] for s in composite]}"
        )
        results += composite

    final = deduplicate_results(results)
    print(f"\nğŸŸ¢ [Step 5] ìµœì¢… ì¶”ì¶œ ê²°ê³¼: {final}")
    return final


def detect_time(text: str, lang: str) -> Optional[str]:
    return next((v for k, v in TIME_KEYWORDS[lang].items() if k in text), None)


def deduplicate_results(results: List[Dict[str, str]]) -> List[Dict[str, str]]:
    seen = set()
    unique = []
    for item in results:
        key = (item["symptom"], item["time"])
        if key not in seen:
            seen.add(key)
            unique.append(item)
    return unique


def handle_composite_symptoms(
    text_ko: str, existing_results: List[Dict[str, str]]
) -> List[Dict[str, str]]:
    """
    'ëª¸ì‚´'ì²˜ëŸ¼ ì—¬ëŸ¬ ì¦ìƒìœ¼ë¡œ ë¶„í•´ ê°€ëŠ¥í•œ ë³µí•© ì¦ìƒì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    composite_symptoms = []

    # ğŸ”¹ ëª¸ì‚´ â†’ ì˜¤í•œ, ê·¼ìœ¡í†µ, í”¼ë¡œ, ë¯¸ì—´
    if "ëª¸ì‚´" in text_ko:
        mapped = ["ì˜¤í•œ", "ê·¼ìœ¡í†µ", "í”¼ë¡œ", "ë¯¸ì—´"]
        for symptom in mapped:
            if not any(r["symptom"] == symptom for r in existing_results):
                composite_symptoms.append(
                    {"symptom": symptom, "time": detect_time(text_ko, "ko")}
                )

    return composite_symptoms
