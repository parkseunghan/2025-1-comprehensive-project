# !pip install focal_loss

# -*- coding: utf-8 -*-
"""
ğŸ“„ ê°ê¸°/ê°ì—¼/ì†Œí™”ê¸°/í˜¸í¡ê¸°/ì‹¬í˜ˆê´€ ì§ˆë³‘ ì˜ˆì¸¡ ëª¨ë¸ (AI ê¸°ë°˜ ìê°€ì§„ë‹¨ìš©)

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì¦ìƒ í‚¤ì›Œë“œ, ìˆ˜ì¹˜/ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‚´ê³¼ ì§ˆí™˜ì„ ì˜ˆì¸¡í•˜ëŠ”
í•˜ì´ë¸Œë¦¬ë“œ AI ëª¨ë¸(coarse-to-fine êµ¬ì¡°)ì„ í•™ìŠµí•˜ê³  ì¶”ë¡ í•˜ëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ í¬í•¨í•©ë‹ˆë‹¤.

âœ… ì£¼ìš” ê¸°ëŠ¥:
- SBERT ì„ë² ë”© ê¸°ë°˜ í…ìŠ¤íŠ¸ ì²˜ë¦¬
- MLP ê¸°ë°˜ ìˆ˜ì¹˜/ì¹´í…Œê³ ë¦¬ ì •ë³´ í†µí•©
- ê°ê¸°/ê°ì—¼/ì†Œí™”ê¸°/í˜¸í¡ê¸°/ì‹¬í˜ˆê´€ coarse ë¶„ë¥˜
- ê° coarse ê·¸ë£¹ì— ëŒ€í•œ fine ëª¨ë¸ ë¶„ê¸° í•™ìŠµ
- ìœ„í—˜ë„ ì ìˆ˜ ê³„ì‚° ë° ì‘ê¸‰ë„ ë¶„ë¥˜
- ëª¨ë¸ ì €ì¥ ë° ì‹¤ì‹œê°„ ì¶”ë¡  ê¸°ëŠ¥ í¬í•¨

ğŸ›  êµ¬ì„±:
- ì „ì²˜ë¦¬: preprocess_features(), preprocess_input()
- coarse ëª¨ë¸: build_model_coarse()
- fine ëª¨ë¸: train_fine_model() with config
- ì¶”ë¡ : predict_disease()
- ì €ì¥: model.save()

ì‘ì„±ì: [ì´ìŠ¹ê²¸]
ìµœì¢… ìˆ˜ì •ì¼: 2025-04-15
"""


# âœ… 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸

# âœ… ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# âœ… Scikit-learn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler, MultiLabelBinarizer
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report, confusion_matrix

# âœ… TensorFlow & Keras
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, Concatenate, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.losses import CategoricalCrossentropy

# âœ… ê¸°íƒ€ ì™¸ë¶€ íŒ¨í‚¤ì§€
from focal_loss import SparseCategoricalFocalLoss

# âœ… 2. ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜
def preprocess_features(df, sbert_embedding_path: str):
    # ğŸ“Œ ê¸°ë³¸ ì •ì œ
    df = df.copy()
    df.drop(columns=['symptom_text'], inplace=True, errors='ignore')
    df["disease_name"] = df["disease_name"].apply(lambda x: x[0] if isinstance(x, list) else x)

    # ğŸ“Œ coarse ë¼ë²¨ ë§¤í•‘
    coarse_map = {
        "ê°ê¸°": ["ê¸‰ì„± ê¸°ê´€ì§€ì—¼", "ê¸‰ì„± ë¹„ì¸ë‘ì—¼", "ê¸‰ì„± ì¸ë‘ì—¼", "ìƒê¸°ë„ ê°ì—¼"],
        "ê°ì—¼": ["ê¸‰ì„± ì¥ì—¼", "ê°„ì—¼", "ìš”ë¡œê°ì—¼"],
        "ì†Œí™”ê¸°": ["ìœ„ì—¼", "ìœ„ì‹ë„ì—­ë¥˜ì§ˆí™˜(GERD)", "ì†Œí™”ì„± ê¶¤ì–‘", "ì·Œì¥ì—¼", "ê³¼ë¯¼ì„± ëŒ€ì¥ì¦í›„êµ°"],
        "í˜¸í¡ê¸°": ["íë ´", "ì²œì‹", "ë§Œì„± íì‡„ì„± íì§ˆí™˜(COPD)"],
        "ì‹¬í˜ˆê´€": ["ì‹¬ë¶€ì „", "í˜‘ì‹¬ì¦", "ë¹ˆí˜ˆ"]
    }

    def map_coarse(name):
        for coarse, fine_list in coarse_map.items():
            if name in fine_list:
                return coarse
        return None

    df["coarse_label"] = df["disease_name"].apply(map_coarse)
    df.dropna(subset=["coarse_label"], inplace=True)
    df.reset_index(drop=True, inplace=True)

    # ğŸ“Œ ìˆ˜ì¹˜í˜• ì •ê·œí™”
    numeric_cols = ["Age", "Height_cm", "Weight_kg", "BMI"]
    scaler = StandardScaler()
    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

    # ğŸ“Œ ë²”ì£¼í˜• ë³€í™˜
    df["chronic_diseases"] = df["chronic_diseases"].apply(lambda x: x.split(',') if isinstance(x, str) and x != "ì—†ìŒ" else [])
    df["medications"] = df["medications"].apply(lambda x: x.split(',') if isinstance(x, str) and x != "ì—†ìŒ" else [])
    gender = df["Gender"].map({"ë‚¨ì„±": 1, "ì—¬ì„±": 0}).fillna(0).values.reshape(-1, 1)

    mlb_chronic = MultiLabelBinarizer()
    mlb_meds = MultiLabelBinarizer()
    chronic_matrix = mlb_chronic.fit_transform(df["chronic_diseases"])
    meds_matrix = mlb_meds.fit_transform(df["medications"])

    # ğŸ“Œ ìµœì¢… MLP í”¼ì²˜
    mlp_features = np.concatenate([
        df[numeric_cols].values,
        gender,
        chronic_matrix,
        meds_matrix
    ], axis=1)

    # ğŸ“Œ SBERT ì„ë² ë”© ë¡œë”© (ë™ê¸°í™”ëœ ìˆœì„œë¡œ ì €ì¥ë˜ì–´ ìˆìŒ)
    sbert_text_features = np.load(sbert_embedding_path)[:len(df)]
    text_features = sbert_text_features.astype(np.float32)

    return df, mlp_features, text_features, scaler, mlb_chronic, mlb_meds

df_raw = pd.read_csv("/content/leaned_train_dataset.csv", encoding="utf-8-sig")
df, mlp_features, text_features, scaler, mlb_chronic, mlb_meds = preprocess_features(df_raw, "leaned_sbert_text_features_final.npy")

# âœ… coarse ë¼ë²¨ ì¸ì½”ë”©
le_coarse = LabelEncoder()
df["coarse_encoded"] = le_coarse.fit_transform(df["coarse_label"])
y_coarse = to_categorical(df["coarse_encoded"])

# âœ… ë°ì´í„° ë¶„í•  (train â†’ val â†’ test)
X_mlp = mlp_features
X_text = text_features

X_train_val_mlp, X_test_mlp, X_train_val_text, X_test_text, y_train_val, y_test = train_test_split(
    X_mlp, X_text, y_coarse,
    test_size=0.15,
    stratify=df["coarse_encoded"],
    random_state=42
)

X_train_mlp, X_val_mlp, X_train_text, X_val_text, y_train, y_val = train_test_split(
    X_train_val_mlp, X_train_val_text, y_train_val,
    test_size=0.1765,  # â†’ 0.15 / 0.85 â‰ˆ 0.1765
    stratify=np.argmax(y_train_val, axis=1),
    random_state=42
)


# âœ… class_weight ê³„ì‚°
y_labels = np.argmax(y_train, axis=1)
weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_labels), y=y_labels)
class_weights_dict = {i: w for i, w in enumerate(weights)}

# âœ… coarse ë¶„ë¥˜ ëª¨ë¸ ì •ì˜
def build_model_coarse(mlp_dim, text_dim, output_dim):
    mlp_input = Input(shape=(mlp_dim,), name='mlp_input')
    x1 = Dense(128, activation='relu')(mlp_input)
    x1 = BatchNormalization()(x1)
    x1 = Dropout(0.3)(x1)

    text_input = Input(shape=(text_dim,), name='bert_input')
    x2 = Dense(256, activation='relu')(text_input)
    x2 = Dropout(0.3)(x2)
    x2 = Dense(128, activation='relu')(x2)

    combined = Concatenate()([x1, x2])
    x = Dense(256, activation='relu')(combined)
    x = Dropout(0.4)(x)
    x = Dense(128, activation='relu')(x)
    output = Dense(output_dim, activation='softmax')(x)

    model = Model(inputs=[mlp_input, text_input], outputs=output)
    model.compile(
        optimizer=Adam(learning_rate=0.0005),
        loss=CategoricalCrossentropy(label_smoothing=0.1),
        metrics=['accuracy']
    )
    return model

# âœ… coarse ëª¨ë¸ í•™ìŠµ
model_coarse = build_model_coarse(
    mlp_dim=mlp_features.shape[1],
    text_dim=text_features.shape[1],
    output_dim=y_coarse.shape[1]
)

model_coarse.fit(
    [X_train_mlp, X_train_text], y_train,
    validation_data=([X_val_mlp, X_val_text], y_val),
    epochs=50,
    batch_size=64,
    callbacks=[EarlyStopping(patience=5, restore_best_weights=True)],
    class_weight=class_weights_dict,
    verbose=1
)

# âœ… coarse ëª¨ë¸ í‰ê°€
y_pred_proba = model_coarse.predict([X_test_mlp, X_test_text])
y_pred = np.argmax(y_pred_proba, axis=1)
y_true = np.argmax(y_test, axis=1)

# ğŸ“Š í‰ê°€ ë¦¬í¬íŠ¸
print("\nğŸ“Š Classification Report:")
print(classification_report(y_true, y_pred, target_names=le_coarse.classes_))

# ğŸ” Confusion Matrix
conf_mat = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',
            xticklabels=le_coarse.classes_,
            yticklabels=le_coarse.classes_)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - Coarse")
plt.show()

"""fine ëª¨ë¸ ë°˜ë³µ"""
# ğŸ”¹ coarse ê·¸ë£¹ë³„ ì„¤ì • íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°

with open("fine_config.json", "r", encoding="utf-8") as f:
    fine_config = json.load(f)

loss_map = {
    "categorical_crossentropy": "categorical_crossentropy",
    "focal": SparseCategoricalFocalLoss(gamma=2.0)
}

def train_fine_model(group_name, df, mlp_features, text_features):
    config = fine_config[group_name]
    df_sub = df[df["coarse_label"] == group_name].copy()
    df_sub["fine_label"] = df_sub["disease_name"]

    # ğŸ”¹ ë¼ë²¨ ì¸ì½”ë”©
    le_fine = LabelEncoder()
    df_sub["fine_encoded"] = le_fine.fit_transform(df_sub["fine_label"])

    # ğŸ”¹ ë¼ë²¨ í˜•ì‹ ì§€ì •
    if config["output_type"] == "onehot":
        y = to_categorical(df_sub["fine_encoded"])
    else:
        y = df_sub["fine_encoded"].values  # ê°ì—¼ ê·¸ë£¹ focalìš©

    # ğŸ”¹ í”¼ì²˜ êµ¬ì„±
    idx = df_sub.index
    X_mlp = mlp_features[idx]
    X_text = text_features[idx]

    # ğŸ”¹ ë°ì´í„° ë¶„í• 
    X_train_mlp, X_val_mlp, X_train_text, X_val_text, y_train, y_val = train_test_split(
        X_mlp, X_text, y,
        test_size=0.15,
        stratify=df_sub["fine_encoded"],
        random_state=42
    )

    # ğŸ”¹ class weight
    weights = compute_class_weight(class_weight='balanced',
                                   classes=np.unique(df_sub["fine_encoded"]),
                                   y=df_sub["fine_encoded"])
    class_weights_dict = {i: w for i, w in enumerate(weights)}

    # ğŸ”¹ ëª¨ë¸ ì •ì˜
    mlp_input = Input(shape=(X_mlp.shape[1],))
    x1 = Dense(64, activation='relu')(mlp_input)
    x1 = Dropout(0.2)(x1)

    text_input = Input(shape=(X_text.shape[1],))
    x2 = Dense(128, activation='relu')(text_input)
    x2 = Dropout(0.3)(x2)

    x = Concatenate()([x1, x2])
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.3)(x)
    output = Dense(y.shape[1] if config["output_type"] == "onehot" else len(le_fine.classes_), activation='softmax')(x)

    model = Model(inputs=[mlp_input, text_input], outputs=output)
    model.compile(
        optimizer=Adam(0.0005),
        loss=loss_map[config["loss"]],
        metrics=["accuracy"]
    )

    # ğŸ”¹ í•™ìŠµ
    model.fit(
        [X_train_mlp, X_train_text], y_train,
        validation_data=([X_val_mlp, X_val_text], y_val),
        epochs=30,
        batch_size=64,
        callbacks=[EarlyStopping(patience=5, restore_best_weights=True)],
        class_weight=class_weights_dict,
        verbose=1
    )

    # ğŸ”¹ í‰ê°€
    y_pred = np.argmax(model.predict([X_val_mlp, X_val_text]), axis=1)
    y_true = np.argmax(y_val, axis=1) if config["output_type"] == "onehot" else y_val

    print(f"\nğŸ“Š {group_name} ê·¸ë£¹ fine ë¶„ë¥˜ ê²°ê³¼:")
    print(classification_report(y_true, y_pred, target_names=le_fine.classes_))

    # ğŸ”¹ ëª¨ë¸, ì¸ì½”ë” ë°˜í™˜
    return model, le_fine

fine_models = {}
fine_label_encoders = {}

for group in fine_config:
    model, encoder = train_fine_model(group, df, mlp_features, text_features)
    fine_models[group] = model
    fine_label_encoders[group] = encoder

# ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_input(input_dict, scaler, mlb_chronic, mlb_meds, sbert_model):
    gender_map = {"ë‚¨ì„±": 1, "ì—¬ì„±": 0}

    # SBERT ì„ë² ë”©
    text_embedding = sbert_model.encode([input_dict["symptom_keywords"]])[0].reshape(1, -1)

    # ìˆ˜ì¹˜í˜• ìŠ¤ì¼€ì¼ë§
    num_input = np.array([
        input_dict["Age"],
        input_dict["Height_cm"],
        input_dict["Weight_kg"],
        input_dict["BMI"]
    ]).reshape(1, -1)
    num_scaled = scaler.transform(num_input)

    # ë²”ì£¼í˜• ì²˜ë¦¬
    gender = np.array([[gender_map.get(input_dict["Gender"], 0)]])
    chronic_bin = mlb_chronic.transform([input_dict["chronic_diseases"]])
    meds_bin = mlb_meds.transform([input_dict["medications"]])

    # ìµœì¢… ì…ë ¥
    mlp_input = np.concatenate([num_scaled, gender, chronic_bin, meds_bin], axis=1)
    return mlp_input, text_embedding

# ìœ„í—˜ë„ ê³„ì‚° í•¨ìˆ˜
def calculate_risk_score(input_dict, confidence):
    W1, W2, W3, W4, W5, W6 = 1, 1, 1, 1, 1, 1

    # ìœ„í—˜ ìš”ì†Œ ê³„ì‚°
    S = 1.0
    risk_diseases = ["ê³ í˜ˆì••", "ë‹¹ë‡¨", "ì‹¬ë¶€ì „", "í˜‘ì‹¬ì¦"]
    C = 1.2 if any(d in input_dict["chronic_diseases"] for d in risk_diseases) else 1.0
    A = 1.2 if input_dict["Age"] >= 60 else 1.0
    G = 1.0
    B = 1.3 if input_dict["BMI"] < 18.5 or input_dict["BMI"] >= 25 else 1.0
    risk_meds = ["ë©´ì—­ì–µì œì œ", "í•­ìƒì œ"]
    M = 1.1 if any(m in input_dict["medications"] for m in risk_meds) else 1.0

    score = confidence * (W1*S + W2*C + W3*A + W4*G + W5*B + W6*M)
    score = round(score, 2)

    # ë“±ê¸‰ ë¶„ë¥˜
    if score < 1.5:
        level, guide = "ë‚®ìŒ", "í˜„ì¬ ìƒíƒœì—ì„œ ìê°€ ê´€ë¦¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤."
    elif score < 3.0:
        level, guide = "ë³´í†µ", "ì „ë¬¸ê°€ ìƒë‹´ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
    elif score < 4.5:
        level, guide = "ë†’ìŒ", "ë¹ ë¥¸ ë³‘ì› ë°©ë¬¸ì´ í•„ìš”í•©ë‹ˆë‹¤."
    else:
        level, guide = "ì‘ê¸‰", "ì¦‰ì‹œ ì‘ê¸‰ì‹¤ ë°©ë¬¸ì´ í•„ìš”í•©ë‹ˆë‹¤. 119ì— ì—°ë½í•˜ì„¸ìš”."

    return score, level, guide

# í†µí•© ì¶”ë¡  í•¨ìˆ˜
def predict_disease(input_dict, model_coarse, fine_models, le_coarse, fine_label_encoders,
                    scaler, mlb_chronic, mlb_meds, sbert_model):
    mlp_input, text_input = preprocess_input(input_dict, scaler, mlb_chronic, mlb_meds, sbert_model)

    # coarse ì˜ˆì¸¡
    coarse_probs = model_coarse.predict([mlp_input, text_input])
    coarse_idx = np.argmax(coarse_probs)
    coarse_label = le_coarse.inverse_transform([coarse_idx])[0]

    # fine ì˜ˆì¸¡
    fine_model = fine_models[coarse_label]
    fine_le = fine_label_encoders[coarse_label]
    fine_probs = fine_model.predict([mlp_input, text_input])
    fine_idx = np.argmax(fine_probs)
    fine_label = fine_le.inverse_transform([fine_idx])[0]
    confidence = round(float(fine_probs[0][fine_idx]), 4)

    # ìœ„í—˜ë„ ê³„ì‚°
    risk_score, risk_level, recommendation = calculate_risk_score(input_dict, confidence)

    return {
    "coarse_label": coarse_label,
    "fine_prediction": fine_label,
    "confidence": confidence,
    "risk_score": risk_score,
    "risk_level": risk_level,
    "recommendation": recommendation
}


sample = {
    "symptom_keywords": "ê¸°ì¹¨, ê°€ë˜, ê°€ìŠ´ ë‹µë‹µí•¨",
    "Age": 50, "Gender": "ë‚¨ì„±", "Height_cm": 175, "Weight_kg": 80, "BMI": 26.1,
    "chronic_diseases": ["ê³ í˜ˆì••"],
    "medications": ["í•­ìƒì œ"]
}

# sbert_model ì„ ì–¸ í•„ìš”!
from sentence_transformers import SentenceTransformer
sbert_model = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")


result = predict_disease(
    sample,
    model_coarse,
    fine_models,
    le_coarse,
    fine_label_encoders,
    scaler,
    mlb_chronic,
    mlb_meds,
    sbert_model
)

print(result)

# coarse ëª¨ë¸ ì €ì¥
model_coarse.save("model_coarse.h5")

# fine ëª¨ë¸ ì €ì¥
for group, model in fine_models.items():
    model.save(f"model_fine_{group}.h5")

